{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eceb92a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-username/your-repo/blob/main/your-notebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9dadf",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Overview & Learning Outcomes\n",
    "\n",
    "**Learning objectives**\n",
    "- Explain the classical NLP pipeline: preprocessing → representation → features → modeling → evaluation.\n",
    "- Use spaCy for tokenization, stopword handling, and lemmatization.\n",
    "- Build Bag-of-Words and TF-IDF representations (with unigrams/bigrams).\n",
    "- Extract POS tags and named entities.\n",
    "- Train simple text classifiers and evaluate them (accuracy, precision/recall/F1, confusion matrix).\n",
    "- (Optional) Visualize TF-IDF spaces with TruncatedSVD.\n",
    "\n",
    "This notebook focuses on **traditional text processing** methods that work well with classical ML models such as\n",
    "Naive Bayes, Logistic Regression, and SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a3280",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Setup & Imports\n",
    "\n",
    "**Learning objectives**\n",
    "- Ensure required libraries are installed and imported.\n",
    "- Load `en_core_web_sm` (download if not present).\n",
    "- Keep runtime light and deterministic.\n",
    "\n",
    "> Notes: Use matplotlib only for plots (no seaborn). Each figure is a single plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e204beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Guarded installs (run once if needed)\n",
    "# import sys, subprocess, importlib\n",
    "\n",
    "# def ensure(pkg):\n",
    "#     try:\n",
    "#         importlib.import_module(pkg)\n",
    "#     except ImportError:\n",
    "#         print(\"Installing\", pkg)\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg])\n",
    "\n",
    "# for pkg in [\"spacy\", \"scikit-learn\", \"matplotlib\", \"pandas\", \"numpy\"]:\n",
    "#     ensure(pkg)\n",
    "\n",
    "import random, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy.cli import download as spacy_download\n",
    "\n",
    "# Determinism where possible\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Load or download small English model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    print(\"Downloading spaCy en_core_web_sm...\")\n",
    "    spacy_download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"spaCy version:\", spacy.__version__)\n",
    "print(\"Model loaded:\", nlp.meta.get(\"name\", \"unknown\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4d778",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Text Preprocessing with spaCy\n",
    "\n",
    "**Learning objectives**\n",
    "- Tokenize, lowercase, remove stopwords, and lemmatize.\n",
    "- Build a small, in-notebook corpus and inspect token frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A tiny corpus across 3 topics (tech, travel, food) + mild noise\n",
    "docs = [\n",
    "    # tech\n",
    "    \"Machine learning improves systems automatically from data.\",\n",
    "    \"C++ and Python interoperate; GPUs accelerate deep learning workloads.\",\n",
    "    \"Read our API docs at https://example.com/docs for usage notes.\",\n",
    "    # travel\n",
    "    \"Bali beaches were amazing and the sunsets were unforgettable!\",\n",
    "    \"We booked flights via email and chatted with @travel_helper.\",\n",
    "    \"Paris in spring is not too crowded; museums were delightful.\",\n",
    "    # food\n",
    "    \"The ramen was so good, but not cheap. I'd still recommend it!\",\n",
    "    \"Try our menu and promotions at http://noodles.example/ — so tasty!\",\n",
    "    \"I dislike overly sweet desserts; they're not my style.\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"tech\",\"tech\",\"tech\",\n",
    "    \"travel\",\"travel\",\"travel\",\n",
    "    \"food\",\"food\",\"food\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"text\": docs, \"label\": labels})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63b5e9",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenization, Stopwords, Lemmatization\n",
    "\n",
    "We implement a small utility that:\n",
    "- lowercases\n",
    "- optionally removes stopwords\n",
    "- optionally lemmatizes\n",
    "- keeps only alphabetic tokens when desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70806b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def spacy_tokenize(text, *, lemmatize=True, remove_stopwords=True, lower=True, keep_alpha=True):\n",
    "    doc = nlp(text)\n",
    "    out = []\n",
    "    for t in doc:\n",
    "        tok = t.lemma_ if lemmatize else t.text\n",
    "        if lower:\n",
    "            tok = tok.lower()\n",
    "        if keep_alpha and not tok.isalpha():\n",
    "            continue\n",
    "        if remove_stopwords and tok in STOP_WORDS:\n",
    "            continue\n",
    "        out.append(tok)\n",
    "    return out\n",
    "\n",
    "# Demo on one document\n",
    "sample = df.text.iloc[0]\n",
    "print(\"Original:\", sample)\n",
    "print(\"Processed tokens:\", spacy_tokenize(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa96125",
   "metadata": {},
   "source": [
    "\n",
    "### Token Frequency Histogram\n",
    "\n",
    "We count top terms after basic preprocessing and show a single-plot bar chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a541386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "all_tokens = []\n",
    "for s in df.text:\n",
    "    all_tokens.extend(spacy_tokenize(s))\n",
    "\n",
    "freqs = Counter(all_tokens)\n",
    "top = freqs.most_common(15)\n",
    "labels_bar = [w for w,_ in top]\n",
    "values_bar = [c for _,c in top]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(len(labels_bar)), values_bar)\n",
    "plt.xticks(range(len(labels_bar)), labels_bar, rotation=45, ha=\"right\")\n",
    "plt.title(\"Top tokens after preprocessing\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd34da0",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Statistical Representations: BoW, N-grams, TF-IDF\n",
    "\n",
    "**Learning objectives**\n",
    "- Build CountVectorizer (BoW) and TfidfVectorizer.\n",
    "- Compare unigrams vs bigrams, vocabulary size, and sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa39e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Wrapper for scikit-learn vectorizers\n",
    "def tokenizer_wrapper(text):\n",
    "    return spacy_tokenize(text, lemmatize=True, remove_stopwords=True, lower=True, keep_alpha=True)\n",
    "\n",
    "X_count_uni = CountVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,1))\n",
    "X_count_bi  = CountVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,2))\n",
    "X_tfidf_bi  = TfidfVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,2))\n",
    "\n",
    "Xc_uni = X_count_uni.fit_transform(df.text)\n",
    "Xc_bi  = X_count_bi.fit_transform(df.text)\n",
    "Xt_bi  = X_tfidf_bi.fit_transform(df.text)\n",
    "\n",
    "def density(mat):\n",
    "    return mat.nnz / (mat.shape[0] * mat.shape[1])\n",
    "\n",
    "print(\"BoW unigrams:\", Xc_uni.shape, \"density:\", round(density(Xc_uni), 4))\n",
    "print(\"BoW uni+bi :\", Xc_bi.shape,  \"density:\", round(density(Xc_bi), 4))\n",
    "print(\"TF-IDF uni+bi:\", Xt_bi.shape, \"density:\", round(density(Xt_bi), 4))\n",
    "\n",
    "# Show a few feature names\n",
    "print(\"Sample TF-IDF features:\", X_tfidf_bi.get_feature_names_out()[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba775d",
   "metadata": {},
   "source": [
    "\n",
    "### Inspect Top TF-IDF Terms per Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "feature_names = X_tfidf_bi.get_feature_names_out()\n",
    "\n",
    "def top_k_tfidf(doc_index, k=8):\n",
    "    row = Xt_bi[doc_index].toarray().ravel()\n",
    "    idx = np.argsort(-row)[:k]\n",
    "    return [(feature_names[i], float(row[i])) for i in idx if row[i] > 0]\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Doc {i}:\", top_k_tfidf(i, k=8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a634927",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Linguistic Features: POS & NER (spaCy)\n",
    "\n",
    "**Learning objectives**\n",
    "- Extract part-of-speech tags and named entities.\n",
    "- Understand when these can be useful in traditional pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfec6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent = \"Paris in spring is not too crowded; museums were delightful.\"\n",
    "doc = nlp(sent)\n",
    "\n",
    "print(\"POS tags (token, POS, head):\")\n",
    "for t in doc:\n",
    "    print(f\"{t.text:12s} {t.pos_:6s} -> {t.head.text}\")\n",
    "\n",
    "print(\"\\nNamed Entities (text, label):\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45650364",
   "metadata": {},
   "source": [
    "\n",
    "# 6. From Features to Models: A Simple Text Classifier\n",
    "\n",
    "**Learning objectives**\n",
    "- Train two classical pipelines and compare:\n",
    "  1) CountVectorizer(1,2) + LogisticRegression\n",
    "  2) TfidfVectorizer(1,2) + LinearSVC\n",
    "- Evaluate with accuracy, precision, recall, F1 (macro) and a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce96e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.text, df.label, test_size=0.33, random_state=7, stratify=df.label\n",
    ")\n",
    "\n",
    "pipe1_vec = CountVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,2))\n",
    "X1_train = pipe1_vec.fit_transform(X_train)\n",
    "X1_test  = pipe1_vec.transform(X_test)\n",
    "clf1 = LogisticRegression(max_iter=1000)\n",
    "clf1.fit(X1_train, y_train)\n",
    "p1 = clf1.predict(X1_test)\n",
    "\n",
    "pipe2_vec = TfidfVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,2))\n",
    "X2_train = pipe2_vec.fit_transform(X_train)\n",
    "X2_test  = pipe2_vec.transform(X_test)\n",
    "clf2 = LinearSVC()\n",
    "clf2.fit(X2_train, y_train)\n",
    "p2 = clf2.predict(X2_test)\n",
    "\n",
    "def report(y_true, y_pred, name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"{name} -> acc={acc:.3f}, P={pr:.3f}, R={rc:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "print(\"Results on test split:\")\n",
    "report(y_test, p1, \"BoW+LR\")\n",
    "report(y_test, p2, \"TF-IDF+LinearSVC\")\n",
    "\n",
    "# Confusion matrix for the better model (pick p2 here for demo)\n",
    "labels_sorted = sorted(df.label.unique())\n",
    "cm = confusion_matrix(y_test, p2, labels=labels_sorted)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.xticks(range(len(labels_sorted)), labels_sorted, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(labels_sorted)), labels_sorted)\n",
    "plt.title(\"Confusion Matrix (TF-IDF + LinearSVC)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad140c",
   "metadata": {},
   "source": [
    "\n",
    "# 7. Dimensionality Reduction & Visualization (Optional)\n",
    "\n",
    "**Learning objectives**\n",
    "- Reduce TF-IDF vectors to 2D with TruncatedSVD (LSA).\n",
    "- Scatter plot documents colored by class (matplotlib defaults).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf639c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "Xt_all = X_tfidf_bi.fit_transform(df.text)\n",
    "svd = TruncatedSVD(n_components=2, random_state=7)\n",
    "Z = svd.fit_transform(Xt_all)\n",
    "\n",
    "# Map labels to integers for color grouping (matplotlib default color cycle)\n",
    "label_to_int = {lab:i for i,lab in enumerate(sorted(df.label.unique()))}\n",
    "colors = [label_to_int[l] for l in df.label]\n",
    "\n",
    "plt.figure()\n",
    "for i, (x, y) in enumerate(Z):\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x+0.01, y+0.01, df.label[i], fontsize=8)\n",
    "plt.title(\"2D projection of TF-IDF (TruncatedSVD)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d510b",
   "metadata": {},
   "source": [
    "\n",
    "# 8. Common Pitfalls & Best Practices\n",
    "\n",
    "- **Data leakage**: Always fit preprocessing/vectorizers on **training** data only.\n",
    "- **Over-aggressive cleaning**: Removing negators or meaningful punctuation can harm sentiment tasks.\n",
    "- **Too many n-grams**: High-order n-grams increase sparsity and overfitting.\n",
    "- **Domain specificity**: Build custom stopword lists; general lists may be inadequate.\n",
    "- **Reproducibility**: Fix seeds and document preprocessing configs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fcd17",
   "metadata": {},
   "source": [
    "\n",
    "# 9. Mini Review & Next Steps\n",
    "\n",
    "**Review questions**\n",
    "1. When is TF-IDF preferable to raw counts?\n",
    "2. Give two cases where removing stopwords is **not** a good idea.\n",
    "3. What trade-offs exist between lemmatization and stemming?\n",
    "4. Why can bigrams improve a classifier? When might they hurt?\n",
    "5. What are some signals POS or NER can add to classical models?\n",
    "\n",
    "**Next steps**\n",
    "- Topic modeling (LDA) on TF-IDF spaces.\n",
    "- Word embeddings (Word2Vec/GloVe) and hybrid pipelines.\n",
    "- Modern encoders (BERT-family) for context-aware features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
