{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eceb92a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/rudyhendrawn/data-course/blob/main/data-prep/text-data/02_general_flow_text_processing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9dadf",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Overview & Learning Outcomes\n",
    "\n",
    "**Learning objectives**\n",
    "- Explain the classical NLP pipeline: preprocessing → representation → features → modeling → evaluation.\n",
    "- Use spaCy for tokenization, stopword handling, and lemmatization.\n",
    "- Build Bag-of-Words and TF-IDF representations (with unigrams/bigrams).\n",
    "- Extract POS tags and named entities.\n",
    "- Train simple text classifiers and evaluate them (accuracy, precision/recall/F1, confusion matrix).\n",
    "- (Optional) Visualize TF-IDF spaces with TruncatedSVD.\n",
    "\n",
    "This notebook focuses on **traditional text processing** methods that work well with classical ML models such as\n",
    "Naive Bayes, Logistic Regression, and SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a3280",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Setup & Imports\n",
    "\n",
    "**Learning objectives**\n",
    "- Ensure required libraries are installed and imported.\n",
    "- Load `en_core_web_sm` (download if not present).\n",
    "- Keep runtime light and deterministic.\n",
    "\n",
    "> Notes: Use matplotlib only for plots (no seaborn). Each figure is a single plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e204beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarded installs (run once if needed)\n",
    "# import sys, subprocess, importlib\n",
    "\n",
    "# def ensure(pkg):\n",
    "#     try:\n",
    "#         importlib.import_module(pkg)\n",
    "#     except ImportError:\n",
    "#         print(\"Installing\", pkg)\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg])\n",
    "\n",
    "# for pkg in [\"spacy\", \"scikit-learn\", \"matplotlib\", \"pandas\", \"numpy\"]:\n",
    "#     ensure(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a45712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy.cli import download as spacy_download\n",
    "\n",
    "# Determinism where possible\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Load or download small English model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    print(\"Downloading spaCy en_core_web_sm...\")\n",
    "    spacy_download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"spaCy version:\", spacy.__version__)\n",
    "print(\"Model loaded:\", nlp.meta.get(\"name\", \"unknown\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4d778",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Text Preprocessing with spaCy\n",
    "\n",
    "**Learning objectives**\n",
    "- Tokenize, lowercase, remove stopwords, and lemmatize.\n",
    "- Build a small, in-notebook corpus and inspect token frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A tiny corpus across 3 topics (tech, travel, food) + mild noise\n",
    "docs = [\n",
    "    # tech\n",
    "    \"Machine learning improves systems automatically from data.\",\n",
    "    \"C++ and Python interoperate; GPUs accelerate deep learning workloads.\",\n",
    "    \"Read our API docs at https://example.com/docs for usage notes.\",\n",
    "    # travel\n",
    "    \"Bali beaches were amazing and the sunsets were unforgettable!\",\n",
    "    \"We booked flights via email and chatted with @travel_helper.\",\n",
    "    \"Paris in spring is not too crowded; museums were delightful.\",\n",
    "    # food\n",
    "    \"The ramen was so good, but not cheap. I'd still recommend it!\",\n",
    "    \"Try our menu and promotions at http://noodles.example/ — so tasty!\",\n",
    "    \"I dislike overly sweet desserts; they're not my style.\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"tech\",\"tech\",\"tech\",\n",
    "    \"travel\",\"travel\",\"travel\",\n",
    "    \"food\",\"food\",\"food\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"text\": docs, \"label\": labels})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63b5e9",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenization, Stopwords, Lemmatization\n",
    "\n",
    "We implement a small utility that:\n",
    "- lowercases\n",
    "- optionally removes stopwords\n",
    "- optionally lemmatizes\n",
    "- keeps only alphabetic tokens when desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70806b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def spacy_tokenize(text, *, lemmatize=True, remove_stopwords=True, lower=True, keep_alpha=True):\n",
    "    doc = nlp(text)\n",
    "    out = []\n",
    "    for t in doc:\n",
    "        tok = t.lemma_ if lemmatize else t.text\n",
    "        if lower:\n",
    "            tok = tok.lower()\n",
    "        if keep_alpha and not tok.isalpha():\n",
    "            continue\n",
    "        if remove_stopwords and tok in STOP_WORDS:\n",
    "            continue\n",
    "        out.append(tok)\n",
    "    return out\n",
    "\n",
    "# Demo on one document\n",
    "sample = df.text.iloc[0]\n",
    "print(\"Original:\", sample)\n",
    "print(\"Processed tokens:\", spacy_tokenize(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa96125",
   "metadata": {},
   "source": [
    "\n",
    "### Token Frequency Histogram\n",
    "\n",
    "We count top terms after basic preprocessing and show a single-plot bar chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a541386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tokens = []\n",
    "for s in df.text:\n",
    "    all_tokens.extend(spacy_tokenize(s))\n",
    "\n",
    "freqs = Counter(all_tokens)\n",
    "top = freqs.most_common(15)\n",
    "labels_bar = [w for w,_ in top]\n",
    "values_bar = [c for _,c in top]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(len(labels_bar)), values_bar)\n",
    "plt.xticks(range(len(labels_bar)), labels_bar, rotation=45, ha=\"right\")\n",
    "plt.title(\"Top tokens after preprocessing\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd34da0",
   "metadata": {},
   "source": [
    "# 4. Statistical Representations: BoW, N-grams, TF-IDF\n",
    "\n",
    "**Learning objectives**\n",
    "- Build CountVectorizer (Bag-of-Words, BoW) to create sparse matrices of word counts.\n",
    "- Build TfidfVectorizer to weight terms by their importance in documents relative to the corpus.\n",
    "- Compare unigrams (single words) vs. bigrams (pairs of words) in terms of vocabulary size, sparsity, and model performance.\n",
    "- Understand how n-grams capture context and reduce ambiguity in text representations.\n",
    "\n",
    "These representations convert raw text into numerical features suitable for machine learning algorithms. \n",
    "\n",
    "**Bag-of-Words (BoW)**: A simple vectorization method that represents each document as a sparse vector of word counts, ignoring word order, grammar, and semantics. For example, in a corpus, the word \"learning\" might appear in multiple documents, and BoW captures its frequency per document. This leads to high-dimensional, sparse matrices that work well for basic classifiers but can suffer from overfitting on noisy or irrelevant terms.\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)**: Builds on BoW by weighting terms based on their importance. Term Frequency (TF) measures how often a word appears in a document, while Inverse Document Frequency (IDF) down-weights common words across the corpus (e.g., \"the\" or \"and\" get lower scores). The formula is TF * log(N / DF), where N is the total documents and DF is documents containing the term. This highlights rare, informative terms, improving relevance for tasks like classification or search.\n",
    "\n",
    "**N-grams** extend these by including sequences of words (e.g., bigrams like \"machine learning\"), capturing some context and reducing ambiguity (e.g., distinguishing \"hot dog\" from \"hot\" and \"dog\" separately). However, they increase vocabulary size and sparsity, requiring careful tuning to balance expressiveness and computational cost. In practice, unigrams are fast but context-poor, while bigrams add nuance at the expense of higher dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa39e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Wrapper for scikit-learn vectorizers\n",
    "def tokenizer_wrapper(text):\n",
    "    return spacy_tokenize(text, lemmatize=True, remove_stopwords=True, lower=True, keep_alpha=True)\n",
    "\n",
    "X_count_uni = CountVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,1))\n",
    "X_count_bi  = CountVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,2))\n",
    "X_tfidf_bi  = TfidfVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,2))\n",
    "\n",
    "Xc_uni = X_count_uni.fit_transform(df.text)\n",
    "Xc_bi  = X_count_bi.fit_transform(df.text)\n",
    "Xt_bi  = X_tfidf_bi.fit_transform(df.text)\n",
    "\n",
    "def density(mat):\n",
    "    return mat.nnz / (mat.shape[0] * mat.shape[1])\n",
    "\n",
    "print(\"BoW unigrams:\", Xc_uni.shape, \"density:\", round(density(Xc_uni), 4))\n",
    "print(\"BoW uni+bi :\", Xc_bi.shape,  \"density:\", round(density(Xc_bi), 4))\n",
    "print(\"TF-IDF uni+bi:\", Xt_bi.shape, \"density:\", round(density(Xt_bi), 4))\n",
    "\n",
    "# Show a few feature names\n",
    "print(\"Sample TF-IDF features:\", X_tfidf_bi.get_feature_names_out()[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba775d",
   "metadata": {},
   "source": [
    "\n",
    "### Inspect Top TF-IDF Terms per Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = X_tfidf_bi.get_feature_names_out()\n",
    "\n",
    "def top_k_tfidf(doc_index, k=8):\n",
    "    row = Xt_bi[doc_index].toarray().ravel()\n",
    "    idx = np.argsort(-row)[:k]\n",
    "    return [(feature_names[i], float(row[i])) for i in idx if row[i] > 0]\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Doc {i}:\", top_k_tfidf(i, k=8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a634927",
   "metadata": {},
   "source": [
    "# 5. Linguistic Features: POS & NER (spaCy)\n",
    "\n",
    "**Learning objectives**\n",
    "- Extract part-of-speech tags and named entities.\n",
    "- Understand when these can be useful in traditional pipelines.\n",
    "\n",
    "**Part-of-Speech (POS)** tagging assigns grammatical categories (e.g., noun, verb, adjective, adverb) to each word in a sentence, helping to understand syntactic structure. \n",
    "\n",
    "**Named Entity Recognition (NER)** identifies and classifies named entities such as persons, organizations, locations, dates, and monetary values into predefined categories.\n",
    "\n",
    "These features can enhance traditional text processing pipelines by providing linguistic context beyond raw words. For instance, POS tags can be used to filter or count specific grammatical elements (e.g., adjectives for sentiment analysis), while NER can extract key entities for tasks like information retrieval or entity-based classification. They are particularly useful in domains where syntax or entities carry semantic weight, such as legal text analysis or news categorization, but may add complexity and require careful integration to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfec6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Paris in spring is not too crowded; museums were delightful.\"\n",
    "doc = nlp(sent)\n",
    "\n",
    "print(\"POS tags (token, POS, head):\")\n",
    "for t in doc:\n",
    "    print(f\"{t.text:12s} {t.pos_:6s} -> {t.head.text}\")\n",
    "\n",
    "print(\"\\nNamed Entities (text, label):\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45650364",
   "metadata": {},
   "source": [
    "# 6. From Features to Models: A Simple Text Classifier\n",
    "\n",
    "**Learning objectives**\n",
    "- Train two classical pipelines and compare:\n",
    "  1) `CountVectorizer(1,2)` + `LogisticRegression`\n",
    "  2) `TfidfVectorizer(1,2)` + `LinearSVC`\n",
    "\n",
    "- Evaluate with accuracy, precision, recall, F1 (macro) and a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce96e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.text, df.label, test_size=0.33, random_state=7, stratify=df.label\n",
    ")\n",
    "\n",
    "pipe1_vec = CountVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,2))\n",
    "X1_train = pipe1_vec.fit_transform(X_train)\n",
    "X1_test  = pipe1_vec.transform(X_test)\n",
    "clf1 = LogisticRegression(max_iter=1000)\n",
    "clf1.fit(X1_train, y_train)\n",
    "p1 = clf1.predict(X1_test)\n",
    "\n",
    "pipe2_vec = TfidfVectorizer(tokenizer=tokenizer_wrapper, ngram_range=(1,2))\n",
    "X2_train = pipe2_vec.fit_transform(X_train)\n",
    "X2_test  = pipe2_vec.transform(X_test)\n",
    "clf2 = LinearSVC()\n",
    "clf2.fit(X2_train, y_train)\n",
    "p2 = clf2.predict(X2_test)\n",
    "\n",
    "def report(y_true, y_pred, name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"{name} -> acc={acc:.3f}, P={pr:.3f}, R={rc:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "print(\"Results on test split:\")\n",
    "report(y_test, p1, \"BoW+LR\")\n",
    "report(y_test, p2, \"TF-IDF+LinearSVC\")\n",
    "\n",
    "# Confusion matrix for the better model (pick p2 here for demo)\n",
    "labels_sorted = sorted(df.label.unique())\n",
    "cm = confusion_matrix(y_test, p2, labels=labels_sorted)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.xticks(range(len(labels_sorted)), labels_sorted, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(labels_sorted)), labels_sorted)\n",
    "plt.title(\"Confusion Matrix (TF-IDF + LinearSVC)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad140c",
   "metadata": {},
   "source": [
    "# 7. Dimensionality Reduction & Visualization (Optional)\n",
    "\n",
    "**Learning objectives**\n",
    "- Reduce TF-IDF vectors to 2D with TruncatedSVD (LSA).\n",
    "- Scatter plot documents colored by class (matplotlib defaults).\n",
    "\n",
    "**Dimensionality Reduction Explanation**  \n",
    "Imagine you have a huge spreadsheet with lots of columns (like thousands) representing words in documents—that's your TF-IDF data. It's hard to look at or understand directly. Dimensionality reduction is like squishing that big spreadsheet into a smaller one with just a few columns (say, 2 for a picture), while trying to keep the important patterns.  \n",
    "\n",
    "TruncatedSVD (a type of Singular Value Decomposition) does this by finding the most important \"directions\" in your data and keeping only the top ones. It's great for text data because TF-IDF is often sparse (mostly zeros), and TruncatedSVD handles that well. This helps you visualize documents as points on a graph, where similar ones cluster together, making it easier to see groups or trends.  \n",
    "\n",
    "For example, in our notebook, we use it to plot documents in 2D, colored by their topic (tech, travel, food). It speeds up computers and reduces noise, but it might miss tiny details. Use it for exploring data, not for final predictions. Other options include PCA (for simpler data) or t-SNE (for twisty patterns), but TruncatedSVD is a solid choice for text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf639c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "Xt_all = X_tfidf_bi.fit_transform(df.text)\n",
    "svd = TruncatedSVD(n_components=2, random_state=7)\n",
    "Z = svd.fit_transform(Xt_all)\n",
    "\n",
    "# Map labels to integers for color grouping (matplotlib default color cycle)\n",
    "label_to_int = {lab:i for i,lab in enumerate(sorted(df.label.unique()))}\n",
    "colors = [label_to_int[l] for l in df.label]\n",
    "\n",
    "plt.figure()\n",
    "for i, (x, y) in enumerate(Z):\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x+0.01, y+0.01, df.label[i], fontsize=8)\n",
    "plt.title(\"2D projection of TF-IDF (TruncatedSVD)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d510b",
   "metadata": {},
   "source": [
    "\n",
    "# 8. Common Pitfalls & Best Practices\n",
    "\n",
    "- **Data leakage**: Always fit preprocessing/vectorizers on **training** data only.\n",
    "- **Over-aggressive cleaning**: Removing negators or meaningful punctuation can harm sentiment tasks.\n",
    "- **Too many n-grams**: High-order n-grams increase sparsity and overfitting.\n",
    "- **Domain specificity**: Build custom stopword lists; general lists may be inadequate.\n",
    "- **Reproducibility**: Fix seeds and document preprocessing configs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fcd17",
   "metadata": {},
   "source": [
    "\n",
    "# 9. Mini Review & Next Steps\n",
    "\n",
    "**Review questions**\n",
    "1. When is TF-IDF preferable to raw counts?\n",
    "2. Give two cases where removing stopwords is **not** a good idea.\n",
    "3. What trade-offs exist between lemmatization and stemming?\n",
    "4. Why can bigrams improve a classifier? When might they hurt?\n",
    "5. What are some signals POS or NER can add to classical models?\n",
    "\n",
    "**Next steps**\n",
    "- Topic modeling (LDA) on TF-IDF spaces.\n",
    "- Word embeddings (Word2Vec/GloVe) and hybrid pipelines.\n",
    "- Modern encoders (BERT-family) for context-aware features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
