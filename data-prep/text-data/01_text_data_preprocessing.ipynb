{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf27da46",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/rudyhendrawn/data-course/blob/main/data-prep/text-data/01_text_data_preprocessing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738b0d1",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Overview & Learning Outcomes\n",
    "\n",
    "**Learning objectives**\n",
    "- Understand the role of text preprocessing in a classical NLP pipeline.\n",
    "- Apply practical steps: canonicalization, cleaning, normalization, tokenization, stopword management, lemmatization/stemming, and simple negation/emoji handling.\n",
    "- Build a small, reusable preprocessing pipeline and measure its impact on feature spaces.\n",
    "\n",
    "**What is text preprocessing?**  \n",
    "\n",
    "Text preprocessing is a crucial step in natural language processing (NLP) that converts raw, unstructured text into a clean, consistent format suitable for machine learning models. It addresses common issues like inconsistencies in encoding, noise from HTML tags or URLs, variations in case and punctuation, and linguistic complexities such as inflections or slang. By standardizing text, preprocessing reduces dimensionality, improves model accuracy, and enhances interpretability. However, it must be tailored to the task‚Äîover-aggressive cleaning can remove important signals (e.g., negators in sentiment analysis). Typical pipeline:\n",
    "> Ingestion ‚Üí Canonicalization ‚Üí Structural Cleaning ‚Üí Normalization ‚Üí Tokenization ‚Üí Stopwords ‚Üí Lemma/Stemming ‚Üí (Optional) Negation/Emoji/Slang/Spelling ‚Üí Vectorization ‚Üí Modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84045373",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Setup & Imports\n",
    "\n",
    "**Learning objectives**\n",
    "- Ensure required libraries are installed and imported.\n",
    "- Load `en_core_web_sm` with a safe, guarded download.\n",
    "\n",
    "> Notes:  \n",
    "> - All examples are tiny and deterministic.  \n",
    "> - Charts use matplotlib (single plot per figure, default colors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68fe74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install (guarded). Re-run the cell if installation occurs.\n",
    "# import sys, subprocess, importlib\n",
    "\n",
    "# def pip_install(package):\n",
    "#     try:\n",
    "#         importlib.import_module(package)\n",
    "#     except ImportError:\n",
    "#         print(f\"Installing {package}...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package])\n",
    "\n",
    "# # Required\n",
    "# for pkg in [\"spacy\", \"scikit-learn\", \"regex\", \"matplotlib\", \"pandas\"]:\n",
    "#     pip_install(pkg)\n",
    "\n",
    "# # Optional\n",
    "# for pkg in [\"ftfy\", \"emoji\", \"bs4\", \"nltk\"]:\n",
    "#     try:\n",
    "#         importlib.import_module(pkg)\n",
    "#     except ImportError:\n",
    "#         try:\n",
    "#             subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg])\n",
    "#         except Exception as e:\n",
    "#             print(f\"Optional package {pkg} not installed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d628bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.util import is_package\n",
    "from spacy.cli import download as spacy_download\n",
    "\n",
    "# Seed everything for determinism where possible\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load en_core_web_sm with guarded download\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    print(\"Downloading spaCy model en_core_web_sm...\")\n",
    "    spacy_download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40688ff",
   "metadata": {},
   "source": [
    "# 3. A Tiny Teaching Corpus & Ground Truth Labels\n",
    "\n",
    "**Learning objectives**\n",
    "- Build a miniature corpus with diverse noise patterns for teaching.\n",
    "- Provide optional labels for later comparison.\n",
    "\n",
    "This section introduces a small, curated dataset designed to illustrate common text preprocessing challenges in natural language processing (NLP). The corpus consists of 12 sample documents, grouped into three thematic categories: technology (tech), travel, and food. Each category includes three examples to ensure balance and variety.\n",
    "\n",
    "To simulate real-world text data, we intentionally inject diverse noise patterns that preprocessing steps will address:\n",
    "- **Structural noise**: HTML tags (e.g., `<b>`, `<i>`), URLs (e.g., `https://example.com`), emails (e.g., `travel.agent@example.org`), mentions (e.g., `@trusted_agent`), and hashtags (e.g., `#vacation`, `#noodles`).\n",
    "- **Linguistic noise**: Emojis (e.g., `:)`, `üòã`), emoticons, elongations (e.g., \"Soooo relaxing\"), slang, and contractions (e.g., \"I'm\", \"I'd\").\n",
    "- **Formatting noise**: Smart quotes (e.g., ‚Äú ‚Äù), extra whitespace, and inconsistent punctuation.\n",
    "\n",
    "The corpus is kept tiny (12 documents) for educational purposes, allowing quick experimentation and visualization without computational overhead. Each document is paired with a ground truth label (e.g., \"tech\", \"travel\", \"food\", or \"misc\") to facilitate later evaluations, such as measuring preprocessing impact on classification tasks or feature space dimensionality.\n",
    "\n",
    "By working with this corpus, you'll see how preprocessing transforms raw, noisy text into cleaner, more uniform representations, reducing variability and improving downstream model performance. The examples are deterministic and reproducible, making it ideal for learning and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a35211",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # tech\n",
    "    \"AI systems learn from data to improve performance over time.\",\n",
    "    \"The new GPU accelerates deep-learning workloads; C++ and Python interop is common.\",\n",
    "    \"Visit <b>our docs</b> at https://example.com/docs for API examples & usage.\",\n",
    "    # travel\n",
    "    \"I loved the beaches in Bali!!! Soooo relaxing :) #vacation\",\n",
    "    \"Book flights via email: travel.agent@example.org ‚Äî or DM @trusted_agent\",\n",
    "    \"Paris is great in spring; museums were not crowded.\",\n",
    "    # food\n",
    "    \"This ramen was *so* good, but not cheap. I'd go again! üòã\",\n",
    "    \"Check out our menu & deals at http://noodles.example/menu #noodles\",\n",
    "    \"I dislike overly sweet desserts; they‚Äôre not my style.\",\n",
    "    # mixed/noisy extras\n",
    "    \"‚ÄúSmart-quotes‚Äù and   extra   spaces\tshould be normalized.\",\n",
    "    \"HTML <i>tags</i> should be stripped (or safely handled).\",\n",
    "    \"I'm sooo happppy about this!!!\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"tech\",\"tech\",\"tech\",\n",
    "    \"travel\",\"travel\",\"travel\",\n",
    "    \"food\",\"food\",\"food\",\n",
    "    \"misc\",\"misc\",\"misc\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"text\": corpus, \"label\": labels})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a284ede",
   "metadata": {},
   "source": [
    "# 4. Ingestion & Canonicalization (Unicode, encodings)\n",
    "\n",
    "**Learning objectives**\n",
    "- Normalize Unicode to reduce spurious variability (e.g., smart quotes vs straight quotes).\n",
    "- Fix common mojibake and odd spacing.\n",
    "- Understand the importance of canonicalization in preventing encoding-related errors that can skew NLP models.\n",
    "\n",
    "Canonicalization is the process of converting text into a standard, consistent form to eliminate variations that arise from different encodings, character representations, or formatting quirks. In NLP, this step is essential because raw text data often contains inconsistencies like smart quotes (‚Äú ‚Äù) instead of straight quotes (\" \"), accented characters in multiple forms, or mojibake (garbled text from encoding mismatches, e.g., \"caf√É¬©\" instead of \"caf√©\"). These variations can lead to inflated vocabulary sizes, poor tokenization, and reduced model performance by treating semantically identical strings as different.\n",
    "\n",
    "We primarily use Unicode normalization via Python's `unicodedata` module:\n",
    "- **NFC (Normalization Form Canonical Composition)**: Composes characters into their canonical forms, useful for most text processing to ensure compatibility.\n",
    "- **NFKC (Normalization Form Compatibility Composition)**: Goes further by decomposing compatibility characters (e.g., full-width Latin letters to standard ones), which is often preferred for NLP to fold equivalences and reduce dimensionality.\n",
    "\n",
    "Additionally, we apply light repairs with regex to handle common issues like excessive whitespace, fancy dashes (‚Äî, ‚Äì), and quote variants. If the `ftfy` (Fix Text For You) library is installed, we leverage it for more robust fixes, such as correcting encoding errors, removing control characters, and normalizing line breaks. Without `ftfy`, we fall back to `unicodedata` and regex, which cover basic cases but may miss complex mojibake.\n",
    "\n",
    "This step ensures that downstream preprocessing (e.g., tokenization) operates on clean, uniform text, improving reproducibility and accuracy in tasks like classification or clustering. Over-normalization can sometimes remove useful signals (e.g., in stylistic analysis), so balance is key‚Äîalways tailor to your task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24743e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ftfy\n",
    "    HAS_FTFY = True\n",
    "except Exception:\n",
    "    HAS_FTFY = False\n",
    "\n",
    "import regex as re\n",
    "import unicodedata\n",
    "\n",
    "def canonicalize_text(s: str, use_ftfy: bool = True) -> str:\n",
    "    \"\"\"Canonicalize text with optional ftfy and Unicode normalization.\"\"\"\n",
    "    if use_ftfy and HAS_FTFY:\n",
    "        s = ftfy.fix_text(s)\n",
    "    # Normalize Unicode (NFKC often good to fold compatibilities)\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    # Replace fancy quotes/dashes with ASCII where sensible\n",
    "    s = s.replace(\"‚Äú\", \"\"\").replace(\"‚Äù\", \"\"\").replace(\"‚Äô\", \"'\").replace(\"‚Äî\", \"-\").replace(\"‚Äì\", \"-\")\n",
    "    # Collapse excessive whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "demo = [\"‚ÄúSmart-quotes‚Äù and   extra   spaces\tshould be normalized.\",\n",
    "        \"I'm sooo happppy about this!!!\"]\n",
    "[canonicalize_text(x) for x in demo]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5818027",
   "metadata": {},
   "source": [
    "# 5. Structural Cleaning (HTML, emails, URLs, mentions, hashtags)\n",
    "\n",
    "**Learning objectives**\n",
    "- Remove or mask structural artifacts that typically do not carry semantic content for many tasks.\n",
    "- Provide configurable behavior (remove vs mask).\n",
    "- Understand the trade-offs between removal and masking to preserve or eliminate specific information based on the NLP task.\n",
    "\n",
    "Structural cleaning targets non-linguistic elements in text that can introduce noise or irrelevant variability, such as HTML tags, URLs, email addresses, social media mentions, and hashtags. These artifacts often stem from web scraping, social media data, or formatted documents and may not contribute to the core semantic meaning in tasks like sentiment analysis or topic modeling. For instance, stripping HTML tags prevents parsing errors and reduces dimensionality, while handling URLs and emails avoids treating them as regular words.\n",
    "\n",
    "The approach can be configured: **removal** deletes these elements entirely, which is aggressive and suitable for general-purpose cleaning where they add no value. **Masking** replaces them with placeholders (e.g., \"__URL__\", \"__EMAIL__\"), preserving their presence for tasks where their existence matters (e.g., detecting spam or link-heavy content). This configurability ensures flexibility‚Äîmasking retains positional information without inflating the vocabulary.\n",
    "\n",
    "In practice, we use libraries like BeautifulSoup for robust HTML stripping and regex for pattern-based removal/masking of URLs, emails, mentions, and hashtags. This step is typically applied after canonicalization to ensure consistent input. Over-removal can lose context (e.g., in social media analysis where hashtags indicate topics), so always align with your task's requirements.\n",
    "\n",
    "> Caution: In some tasks (e.g., link classification), URLs or emails may be informative; prefer masking to removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    HAS_BS4 = True\n",
    "except Exception:\n",
    "    HAS_BS4 = False\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", flags=re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "MENTION_RE = re.compile(r\"@\\w+\")\n",
    "HASHTAG_RE = re.compile(r\"#\\w+\")\n",
    "\n",
    "def strip_html(text: str) -> str:\n",
    "    if HAS_BS4:\n",
    "        return BeautifulSoup(text, \"html.parser\").get_text(\" \")\n",
    "    # Regex fallback (simplistic)\n",
    "    return re.sub(r\"<[^>]+>\", \" \", text)\n",
    "\n",
    "def structural_clean(text: str, mask=True) -> str:\n",
    "    t = strip_html(text)\n",
    "    if mask:\n",
    "        t = URL_RE.sub(\" __URL__ \", t)\n",
    "        t = EMAIL_RE.sub(\" __EMAIL__ \", t)\n",
    "        t = MENTION_RE.sub(\" __MENTION__ \", t)\n",
    "        t = HASHTAG_RE.sub(\" __HASHTAG__ \", t)\n",
    "    else:\n",
    "        t = URL_RE.sub(\" \", t)\n",
    "        t = EMAIL_RE.sub(\" \", t)\n",
    "        t = MENTION_RE.sub(\" \", t)\n",
    "        t = HASHTAG_RE.sub(\" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "samples = [\n",
    "    \"Visit <b>our docs</b> at https://example.com/docs for API examples & usage.\",\n",
    "    \"Book flights via email: travel.agent@example.org ‚Äî or DM @trusted_agent\",\n",
    "    \"Check out our menu & deals at http://noodles.example/menu #noodles\"\n",
    "]\n",
    "[structural_clean(s, mask=True) for s in samples]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b39483",
   "metadata": {},
   "source": [
    "# 6. Normalization (case, accents, punctuation, digits, whitespace)\n",
    "\n",
    "**Learning objectives**\n",
    "- Apply case-folding, accent stripping, and configurable punctuation/digit handling.\n",
    "- Visualize token frequency before/after normalization.\n",
    "\n",
    "Normalization is a key step in text preprocessing that standardizes text to reduce variability and improve consistency for downstream NLP tasks. It addresses inconsistencies in case, accents, punctuation, digits, and whitespace that can inflate vocabulary sizes or cause models to treat similar terms as distinct. For example, \"Apple\" and \"apple\" might be semantically identical in many contexts, but case sensitivity could split them into separate tokens.\n",
    "\n",
    "- **Case-folding**: Converting all text to lowercase (or uppercase) to eliminate case-based variations. This is crucial for tasks like search or classification where case doesn't carry meaning, but should be avoided in cases like named entity recognition where capitalization indicates proper nouns.\n",
    "- **Accent stripping**: Removing diacritical marks (e.g., √© ‚Üí e) using Unicode normalization (NFD to decompose, then filter out combining marks). This reduces dimensionality by treating accented and non-accented forms as equivalent, but may not be suitable for languages where accents change meaning.\n",
    "- **Punctuation and digit handling**: Configurably removing or preserving punctuation and digits. Punctuation often adds noise in bag-of-words models but is vital for tasks like sentiment analysis (e.g., \"not good\" vs \"not good!\"). Digits are typically stripped unless they represent meaningful quantities (e.g., in financial text).\n",
    "- **Whitespace normalization**: Collapsing multiple spaces, tabs, or newlines into single spaces and trimming edges to prevent tokenization artifacts.\n",
    "\n",
    "These transformations are applied after canonicalization and structural cleaning to ensure clean input. The impact can be visualized by comparing token frequencies before and after normalization, often showing reduced vocabulary size and increased sparsity in vectorized representations. Over-normalization can remove useful signals (e.g., in stylistic analysis), so configurations should align with the task‚Äîe.g., keep punctuation for sentiment, strip for topic modeling. Always test on a subset to measure effects on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e811e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize_text(text: str, lower=True, strip_accents=True, keep_punct=False, keep_digits=False) -> str:\n",
    "    t = text\n",
    "    if lower:\n",
    "        t = t.lower()\n",
    "    if strip_accents:\n",
    "        # Decompose into base + diacritics, drop combining marks\n",
    "        t = unicodedata.normalize(\"NFD\", t)\n",
    "        t = \"\".join(ch for ch in t if unicodedata.category(ch) != \"Mn\")\n",
    "        t = unicodedata.normalize(\"NFC\", t)\n",
    "    if not keep_digits:\n",
    "        t = re.sub(r\"\\d+\", \" \", t)\n",
    "    if not keep_punct:\n",
    "        # Remove basic punctuation; keep placeholders like __URL__\n",
    "        t = re.sub(r\"[^\\w\\s]|_\", lambda m: \" \" if not m.group(0).startswith(\"__\") else m.group(0), t)\n",
    "    # Collapse whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "raw_tokens = [w.text for w in nlp(df.text.iloc[0])]\n",
    "norm_example = normalize_text(structural_clean(corpus[2]))\n",
    "print(\"Raw tokens (doc0):\", raw_tokens[:12], \"...\")\n",
    "print(\"Normalized example:\", norm_example)\n",
    "\n",
    "# Simple frequency before/after on the whole corpus\n",
    "before_tokens = [w.text for doc in nlp.pipe(df.text.tolist()) for w in doc if not w.is_space]\n",
    "after_tokens = []\n",
    "for txt in df.text:\n",
    "    t = canonicalize_text(txt)\n",
    "    t = structural_clean(t, mask=True)\n",
    "    t = normalize_text(t, lower=True, strip_accents=True, keep_punct=False, keep_digits=False)\n",
    "    after_tokens.extend([w.text for w in nlp(t) if not w.is_space])\n",
    "\n",
    "def plot_top(freqs, title):\n",
    "    items = freqs.most_common(15)\n",
    "    labels = [k for k, _ in items]\n",
    "    values = [v for _, v in items]\n",
    "    plt.figure()\n",
    "    plt.bar(range(len(labels)), values)\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top(Counter(before_tokens), \"Top tokens BEFORE normalization\")\n",
    "plot_top(Counter(after_tokens), \"Top tokens AFTER normalization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a1716",
   "metadata": {},
   "source": [
    "# 7. Tokenization: Sentence-, Word-, and Rule-based\n",
    "\n",
    "**Learning objectives**\n",
    "- Understand the fundamentals of tokenization in NLP, including its role in breaking down text into manageable units.\n",
    "- Compare sentence-level tokenization (segmenting text into sentences) with word-level tokenization (splitting sentences into words or subwords).\n",
    "- Implement and apply custom tokenizer rules to handle special cases, such as preserving compound terms like `C++` or hyphenated words like `e-mail` as single tokens.\n",
    "- Recognize the trade-offs between rule-based tokenization and more advanced methods (e.g., subword tokenization in transformers).\n",
    "\n",
    "Tokenization is a foundational step in text preprocessing that involves dividing raw text into smaller, meaningful units called tokens. These tokens can represent words, subwords, punctuation, or even entire sentences, depending on the granularity required for the task. Effective tokenization ensures that downstream processes like vectorization, modeling, and analysis operate on consistent, interpretable elements, reducing noise and improving model performance. Poor tokenization can lead to inflated vocabularies, loss of context, or misinterpretation of phrases (e.g., treating \"New York\" as two separate tokens when it should be one entity).\n",
    "\n",
    "### Sentence Tokenization\n",
    "Sentence tokenization, also known as sentence segmentation, splits text into individual sentences. This is crucial for tasks that require understanding document structure, such as summarization, question-answering, or sentiment analysis at the sentence level. Libraries like spaCy use rule-based approaches combined with machine learning models to detect sentence boundaries based on punctuation (e.g., periods, exclamation marks) and linguistic cues (e.g., capitalization after punctuation). For example:\n",
    "- Input: \"I love Paris. It's beautiful!\"\n",
    "- Output: [\"I love Paris.\", \"It's beautiful!\"]\n",
    "\n",
    "Challenges include handling abbreviations (e.g., \"Dr.\" not ending a sentence) or informal text with ellipses. Over-segmentation can fragment related ideas, while under-segmentation might merge unrelated sentences.\n",
    "\n",
    "### Word Tokenization\n",
    "Word tokenization breaks sentences into words, subwords, or tokens, often treating punctuation as separate elements. This is the most common form of tokenization for bag-of-words models or embeddings. SpaCy's tokenizer employs a combination of rules, prefix/suffix patterns, and exception lists to handle complexities like contractions (\"don't\" ‚Üí [\"do\", \"n't\"]) or compound words. For instance:\n",
    "- Input: \"The new GPU accelerates deep-learning workloads.\"\n",
    "- Output: [\"The\", \"new\", \"GPU\", \"accelerates\", \"deep\", \"-\", \"learning\", \"workloads\", \".\"]\n",
    "\n",
    "Advanced variants include subword tokenization (e.g., Byte-Pair Encoding in BERT), which splits rare words into smaller units to manage out-of-vocabulary issues. Rule-based tokenization is fast and deterministic but may struggle with domain-specific jargon or multilingual text.\n",
    "\n",
    "### Custom Rules and Special Cases\n",
    "To enhance tokenization, custom rules can be added to spaCy's tokenizer to preserve specific patterns as single tokens. This prevents over-splitting of meaningful units, such as programming languages (\"C++\"), email formats (\"e-mail\"), or domain terms. For example, adding special cases ensures \"C++\" remains intact instead of being split into \"C\" and \"++\". This is done via `nlp.tokenizer.add_special_case()`, specifying the exact orthography and desired token structure. Custom rules improve accuracy for technical or specialized corpora but require careful tuning to avoid conflicts with general rules.\n",
    "\n",
    "In summary, tokenization bridges raw text and structured data, with sentence tokenization providing high-level structure and word tokenization enabling fine-grained analysis. Custom rules add flexibility for edge cases, but always validate on your dataset to ensure tokens align with semantic intent. Over-reliance on defaults can miss nuances, so iterate and test for your NLP task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7bb0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "# Custom rules: keep \"C++\" and \"e-mail\" intact\n",
    "special_cases = [{\"ORTH\": \"C++\"}, {\"ORTH\": \"e-mail\"}]\n",
    "for case in special_cases:\n",
    "    nlp.tokenizer.add_special_case(case[\"ORTH\"], [case])\n",
    "\n",
    "def spacy_tokenize(doc_text, lemma=False, keep_alpha=True, preserve_case=False):\n",
    "    doc = nlp(doc_text)\n",
    "    tokens = []\n",
    "    for t in doc:\n",
    "        if keep_alpha and not t.text.isalpha() and t.text not in (\"C++\", \"e-mail\"):\n",
    "            continue\n",
    "        tok = t.lemma_ if lemma else t.text\n",
    "        tok = tok if preserve_case else tok.lower()\n",
    "        tokens.append(tok)\n",
    "    return tokens\n",
    "\n",
    "text_ex = \"The new GPU accelerates deep-learning. C++ interop via e-mail is OK!\"\n",
    "print(\"Sentence segmentation:\")\n",
    "for sent in nlp(text_ex).sents:\n",
    "    print(\"-\", sent.text)\n",
    "\n",
    "print(\"\\nWord tokens:\", [t.text for t in nlp(text_ex)])\n",
    "print(\"Custom tokenizer + lemma:\", spacy_tokenize(text_ex, lemma=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4acd5a",
   "metadata": {},
   "source": [
    "# 8. Stopwords: Defaults, Custom Lists, Domain Terms\n",
    "\n",
    "**Learning objectives**\n",
    "- Understand when to remove vs keep stopwords.\n",
    "- Use spaCy defaults and extend with domain-specific lists. Keep negators if desired.\n",
    "\n",
    "Stopwords are common words that appear frequently in text but often carry little semantic value, such as articles (\"the\", \"a\"), prepositions (\"in\", \"on\"), and auxiliary verbs (\"is\", \"be\"). In NLP preprocessing, removing stopwords helps reduce the dimensionality of feature spaces, improves computational efficiency, and focuses models on more informative terms. For example, in topic modeling or search indexing, stopwords can be safely discarded to avoid noise.\n",
    "\n",
    "However, the decision to remove stopwords depends on the task:\n",
    "- **Remove for general tasks**: In bag-of-words models or clustering, where high-frequency common words dilute signals, removal is beneficial.\n",
    "- **Keep for sentiment or context-sensitive tasks**: Stopwords like negators (\"not\", \"no\", \"never\") can flip meanings (e.g., \"not good\" vs. \"good\"). Retaining them preserves sentiment polarity or negation scope.\n",
    "- **Domain considerations**: Default lists may not cover specialized jargon. For instance, in technical texts, words like \"subject\" or \"http\" might be noise and should be added to custom stopword lists.\n",
    "\n",
    "SpaCy provides a built-in English stopword list (`spacy.lang.en.stop_words.STOP_WORDS`), which includes around 300 common words. You can extend this with domain-specific terms (e.g., `{\"subject\", \"re\", \"api\"}` for email corpora) or exclude certain words like negators to customize behavior. Always test on your dataset to ensure removal doesn't strip essential context‚Äîover-removal can harm performance in nuanced tasks like question-answering or sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as EN_STOP\n",
    "\n",
    "DOMAIN_STOPS = {\"subject\", \"re\", \"http\", \"https\", \"api\"}\n",
    "NEGATORS = {\"no\", \"not\", \"never\"}\n",
    "\n",
    "def remove_stopwords(tokens, keep_negators=True):\n",
    "    result = []\n",
    "    for tok in tokens:\n",
    "        if keep_negators and tok in NEGATORS:\n",
    "            result.append(tok)\n",
    "            continue\n",
    "        if tok in EN_STOP or tok in DOMAIN_STOPS:\n",
    "            continue\n",
    "        result.append(tok)\n",
    "    return result\n",
    "\n",
    "toks = spacy_tokenize(\"Paris is not cheap, but it is beautiful!\", lemma=True)\n",
    "print(\"Before:\", toks)\n",
    "print(\"After stopword removal (keep negators):\", remove_stopwords(toks, keep_negators=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114e608",
   "metadata": {},
   "source": [
    "# 9. Lemmatization vs. Stemming: Trade-offs & Demos\n",
    "\n",
    "**Learning objectives**\n",
    "- Compare linguistic lemmatization with algorithmic stemming.\n",
    "- Understand when each is appropriate and their respective trade-offs in NLP preprocessing.\n",
    "\n",
    "Lemmatization and stemming are both techniques used to reduce words to their base forms, helping to normalize text by grouping together inflected or derived forms of a word. This reduces vocabulary size, improves model efficiency, and enhances the ability to match related terms in tasks like search, classification, or clustering. However, they differ significantly in approach, accuracy, and computational cost.\n",
    "\n",
    "### Lemmatization\n",
    "Lemmatization is a linguistic approach that reduces words to their canonical or dictionary form (the lemma), taking into account the word's context, part of speech (POS), and morphological analysis. It uses language-specific rules and often relies on pre-trained models or lexicons to ensure the output is a valid word.\n",
    "\n",
    "- **How it works**: For example, \"running\" (verb) becomes \"run\", \"better\" (comparative adjective) becomes \"good\", and \"studies\" (noun) becomes \"study\". It distinguishes between different meanings based on POS (e.g., \"running\" as a verb vs. \"running\" as a gerund).\n",
    "- **Pros**: More accurate and linguistically sound, producing real words that preserve semantic meaning. Ideal for tasks requiring high precision, such as information retrieval or sentiment analysis.\n",
    "- **Cons**: Computationally intensive, as it requires POS tagging and access to linguistic resources. Slower on large datasets.\n",
    "- **When to use**: In applications where accuracy outweighs speed, like academic research, legal text analysis, or when working with morphologically rich languages.\n",
    "\n",
    "### Stemming\n",
    "Stemming is an algorithmic approach that reduces words to their root form by heuristically removing suffixes and prefixes, often without considering context or POS. It's rule-based and language-agnostic, using algorithms like Porter, Snowball, or Lancaster.\n",
    "\n",
    "- **How it works**: For example, \"running\" becomes \"run\", \"better\" becomes \"bett\" (or similar, depending on the algorithm), and \"studies\" becomes \"studi\". It may produce non-words (stems) that aren't valid dictionary entries.\n",
    "- **Pros**: Fast and lightweight, requiring minimal resources. Effective for reducing dimensionality in large corpora.\n",
    "- **Cons**: Less accurate, as it can over-stem (e.g., \"university\" and \"universe\" both stem to \"univers\") or under-stem, leading to inconsistencies. Ignores linguistic nuances.\n",
    "- **When to use**: In high-volume, real-time applications like web search engines or preliminary data exploration, where speed is prioritized over precision.\n",
    "\n",
    "### Trade-offs and Comparison\n",
    "- **Accuracy vs. Speed**: Lemmatization provides better semantic accuracy but is slower; stemming is quicker but cruder.\n",
    "- **Output Quality**: Lemmatization yields valid words; stemming often results in stems that may not be interpretable.\n",
    "- **Resource Requirements**: Lemmatization needs POS taggers and models (e.g., spaCy); stemming relies on simple rules.\n",
    "- **Task Suitability**: Use lemmatization for nuanced tasks (e.g., topic modeling with semantic coherence); use stemming for broad matching (e.g., keyword extraction in big data).\n",
    "- **Example Comparison**:\n",
    "\t- Word: \"better\", \"running\", \"studies\"\n",
    "\t- Lemmatization (spaCy): \"good\", \"run\", \"study\"\n",
    "\t- Stemming (Porter via NLTK): \"better\", \"run\", \"studi\"\n",
    "\n",
    "In practice, the choice depends on your dataset size, language, and task requirements. For English, spaCy offers robust lemmatization, while NLTK provides stemming options. Always evaluate on a sample to measure impact on vocabulary reduction and model performance.\n",
    "\n",
    "> This demo uses spaCy lemmatization. If NLTK is installed, a stemming example is shown; otherwise, we only print a note.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d257ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences for demonstration\n",
    "examples = [\n",
    "    \"The running ponies are better than studies on horses.\",\n",
    "    \"I am not running to the store for better food.\"\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(examples, 1):\n",
    "    print(f\"\\nExample {i}: '{sentence}'\")\n",
    "    \n",
    "    # Tokenize using spaCy\n",
    "    tokens = spacy_tokenize(sentence, lemma=False, keep_alpha=True, preserve_case=False)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    no_stops = remove_stopwords(tokens, keep_negators=True)\n",
    "    print(\"After stopword removal:\", no_stops)\n",
    "    \n",
    "    # Lemmatize the remaining tokens\n",
    "    lemmas = [w.lemma_ for w in nlp(\" \".join(no_stops))]\n",
    "    print(\"Lemmas:\", lemmas)\n",
    "    \n",
    "    # Stemming (if NLTK available)\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk.stem import SnowballStemmer\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stems = [stemmer.stem(w) for w in no_stops]\n",
    "        print(\"Stems: \", stems)\n",
    "    except Exception as e:\n",
    "        print(\"NLTK not installed for stemming demo (optional).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761d142",
   "metadata": {},
   "source": [
    "# 10. Handling Negation & Contractions\n",
    "\n",
    "**Learning objectives**\n",
    "- Expand common English contractions to standardize text and improve tokenization accuracy.\n",
    "- Implement a simple negation marking scheme to preserve sentiment polarity in downstream tasks like sentiment analysis.\n",
    "\n",
    "Handling negation and contractions is a nuanced step in text preprocessing that addresses linguistic phenomena where word forms can alter meaning or introduce ambiguity. These elements are particularly critical in tasks like sentiment analysis, where subtle changes (e.g., \"not good\" vs. \"good\") can flip polarity, or in information retrieval, where expanded forms ensure consistent matching.\n",
    "\n",
    "### Expanding Contractions\n",
    "Contractions are shortened forms of words created by omitting letters and replacing them with an apostrophe (e.g., \"don't\" for \"do not\"). They are common in informal text like social media or conversational data but can complicate tokenization and normalization. Expanding them converts contractions back to their full forms, reducing variability and ensuring that models treat \"don't\" and \"do not\" equivalently.\n",
    "\n",
    "- **Why expand?** Unexpanded contractions may be split incorrectly during tokenization (e.g., \"don't\" as \"don\" and \"'t\"), leading to inflated vocabularies or missed semantic connections. Expansion standardizes text, making it easier for lemmatization or vectorization.\n",
    "- **Implementation**: Use a dictionary of common contractions (e.g., \"don't\" ‚Üí \"do not\") and apply regex-based replacement to match whole words case-insensitively. This handles variations like \"Don't\" or \"DON'T\" without over-matching partial strings.\n",
    "- **Trade-offs**: Expansion increases text length slightly but improves consistency. For languages with fewer contractions, this step may be optional. Always test on your corpus to avoid introducing artifacts (e.g., in formal texts where contractions are rare).\n",
    "\n",
    "### Negation Marking\n",
    "Negation involves words like \"not\", \"no\", or \"never\" that can invert the meaning of subsequent terms (e.g., \"not happy\" conveys unhappiness). In bag-of-words models, this scope is often lost, causing models to misclassify sentiment. Negation marking appends a suffix (e.g., \"_NEG\") to tokens within the negation scope, explicitly signaling the inversion.\n",
    "\n",
    "- **Why mark negation?** It preserves polarity for sentiment tasks, preventing models from treating \"not good\" as positive. Without marking, vectorized representations might group \"good\" and \"not good\" similarly, harming accuracy.\n",
    "- **Simple scheme**: Identify negators and mark all following tokens until punctuation (e.g., \".\", \"!\", \"?\", \",\", \";\", \":\"). This resets the scope at sentence boundaries or clauses, approximating linguistic negation rules without complex parsing.\n",
    "- **Trade-offs**: This heuristic is lightweight and effective for short texts but may over-mark in complex sentences (e.g., \"I not only like it, but love it\"). For advanced needs, consider dependency parsing. Retain negators themselves to maintain context, and evaluate on labeled data to ensure it boosts performance without noise.\n",
    "\n",
    "In practice, apply contraction expansion before tokenization, and negation marking after stopwords removal to focus on content words. This step enhances model robustness, especially in opinion mining or review analysis, but should be tuned to your task‚Äîskip if negation isn't relevant (e.g., in topic modeling). Always combine with other preprocessing steps for a cohesive pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS = {\n",
    "    \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "    \"can't\": \"can not\", \"won't\": \"will not\", \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "    \"shouldn't\": \"should not\", \"couldn't\": \"could not\", \"wouldn't\": \"would not\",\n",
    "    \"it's\": \"it is\", \"i'm\": \"i am\", \"they're\": \"they are\", \"we're\": \"we are\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text: str) -> str:\n",
    "    def repl(m):\n",
    "        return CONTRACTIONS.get(m.group(0).lower(), m.group(0))\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, CONTRACTIONS.keys())) + r\")\\b\", flags=re.IGNORECASE)\n",
    "    return pattern.sub(repl, text)\n",
    "\n",
    "def mark_negation(tokens):\n",
    "    \"\"\"\n",
    "    Append '_NEG' to tokens that occur after a negation word until punctuation.\n",
    "    Punctuation here is any token matching [. ! ? , ; :]\n",
    "    \"\"\"\n",
    "    marked = []\n",
    "    negate = False\n",
    "    for t in tokens:\n",
    "        if t in {\"not\", \"no\", \"never\"}:\n",
    "            negate = True\n",
    "            marked.append(t)\n",
    "            continue\n",
    "        if re.match(r\"[\\.!\\?,;:]\", t):\n",
    "            negate = False\n",
    "            marked.append(t)\n",
    "            continue\n",
    "        marked.append(t + \"_NEG\" if negate else t)\n",
    "    return marked\n",
    "\n",
    "samples = [\n",
    "    \"I don't like overly sweet desserts, but I do like ramen.\",\n",
    "    \"She isn't happy with the results, but she isn't sad either.\",\n",
    "    \"They can't go now, but they won't stay forever.\",\n",
    "    \"I'm not sure if it's good, but it's not bad.\"\n",
    "]\n",
    "\n",
    "for sample in samples:\n",
    "    expanded = expand_contractions(sample)\n",
    "    toks = spacy_tokenize(expanded, lemma=True, keep_alpha=False)  # keep punctuation to reset negation\n",
    "    print(\"Expanded: \", expanded)\n",
    "    print(\"Tokens:   \", toks)\n",
    "    print(\"Negation: \", mark_negation(toks))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1e6e1",
   "metadata": {},
   "source": [
    "# 11. Emojis, Emoticons, Elongations, and Slang\n",
    "\n",
    "**Learning objectives**\n",
    "- Map emojis to text (if `emoji` installed), handle emoticons and elongations.\n",
    "- Apply a tiny slang dictionary replacement.\n",
    "\n",
    "Emojis, emoticons, elongations, and slang are common elements in informal text, especially from social media, chats, or user-generated content. These can introduce noise or convey nuanced sentiment/emotion that models need to handle appropriately. Preprocessing these elements standardizes them for better NLP performance, reducing variability while preserving meaning where possible. This step is optional but valuable for tasks like sentiment analysis or topic modeling on casual corpora.\n",
    "\n",
    "### Handling Emojis\n",
    "Emojis are pictorial symbols (e.g., üòã for delicious food) that add emotional or contextual cues. Raw emojis can be treated as noise or special characters, leading to tokenization issues. If the `emoji` library is available, we can \"demojize\" them‚Äîconvert to descriptive text (e.g., üòã ‚Üí \":face_savoring_food:\"). This maps visual elements to words, allowing models to process them as tokens. Without the library, emojis are left as-is or removed. This preserves sentiment (e.g., positive emojis in reviews) but increases vocabulary if not handled.\n",
    "\n",
    "### Handling Emoticons\n",
    "Emoticons are text-based facial expressions (e.g., :) for happy, :( for sad) that mimic emotions. They are often inconsistent in format (e.g., :), :-), ;) ). We use regex to detect and replace them with a placeholder like \"__EMOTICON__\", masking their presence without losing the indication of emotion. This prevents over-splitting during tokenization and standardizes representation, useful for emotion detection tasks.\n",
    "\n",
    "### Normalizing Elongations\n",
    "Elongations involve repeated characters for emphasis (e.g., \"Soooo relaxing\" for \"So relaxing\"). These can inflate token uniqueness. We apply regex to limit repeats (e.g., to 2 max), normalizing \"Soooo\" to \"Soo\". This reduces noise while retaining emphasis, improving model generalization without stripping stylistic intent.\n",
    "\n",
    "### Replacing Slang\n",
    "Slang abbreviations (e.g., \"imo\" for \"in my opinion\") are prevalent in informal text. A small dictionary maps them to full forms, standardizing language and aiding comprehension. This is lightweight and domain-specific‚Äîexpand the dict for your corpus. It helps in tasks where precise meaning matters, like opinion mining.\n",
    "\n",
    "These transformations are applied after basic cleaning (e.g., canonicalization) and before tokenization. They enhance consistency in noisy data but should be tuned: e.g., keep emojis for sentiment, remove for factual tasks. Always test on samples to ensure they don't distort meaning. In the demo below, we chain these steps on a sample text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import emoji as emoji_lib\n",
    "    HAS_EMOJI = True\n",
    "except Exception:\n",
    "    HAS_EMOJI = False\n",
    "\n",
    "# Safer, compact emoticon pattern (non-capturing groups)\n",
    "EMOTICON_RE = re.compile(r\"(?:[:;]-?\\)|:-?\\(|:D)\")\n",
    "# Oops: we need correct pattern; rebuild properly:\n",
    "EMOTICON_RE = re.compile(r\"(?:[:;]-?\\)|:-?\\(|:D)\")\n",
    "\n",
    "# Actually, define a clean pattern:\n",
    "EMOTICON_RE = re.compile(r\"(?::|;)(?:-)?(?:\\)|\\()|(?::D)\")\n",
    "\n",
    "# Simpler final pattern:\n",
    "EMOTICON_RE = re.compile(r\"(?:[:;]-?\\))|(?:[:]-?\\()|(?::D)\")\n",
    "\n",
    "# Final truly minimal pattern that works:\n",
    "EMOTICON_RE = re.compile(r\"(?:[:;]-?\\))|(?::-?\\()|(?::D)\")\n",
    "\n",
    "ELONG_RE = re.compile(r\"(.)\\1{2,}\")  # 3+ repeats\n",
    "\n",
    "SLANG = {\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"btw\": \"by the way\"\n",
    "}\n",
    "\n",
    "def handle_emojis(text: str) -> str:\n",
    "    if HAS_EMOJI:\n",
    "        return emoji_lib.demojize(text, language=\"en\")\n",
    "    return text  # fallback: leave as-is with a note\n",
    "\n",
    "def handle_emoticons(text: str) -> str:\n",
    "    return EMOTICON_RE.sub(\" __EMOTICON__ \", text)\n",
    "\n",
    "def normalize_elongations(text: str, max_repeat=2) -> str:\n",
    "    return ELONG_RE.sub(lambda m: m.group(1) * max_repeat, text)\n",
    "\n",
    "def replace_slang(text: str) -> str:\n",
    "    words = text.split()\n",
    "    return \" \".join([SLANG.get(w.lower(), w) for w in words])\n",
    "\n",
    "s = \"I'm sooo happppy about this!!! :) imo\"\n",
    "s1 = handle_emojis(s)\n",
    "s2 = handle_emoticons(s1)\n",
    "s3 = normalize_elongations(s2, max_repeat=2)\n",
    "s4 = replace_slang(s3)\n",
    "print(\"Original: \", s)\n",
    "print(\"Step1 emoji->text:\", s1 if HAS_EMOJI else \"emoji lib not available, skipping mapping\")\n",
    "print(\"Step2 emoticons:  \", s2)\n",
    "print(\"Step3 elongation: \", s3)\n",
    "print(\"Step4 slang:      \", s4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67883eda",
   "metadata": {},
   "source": [
    "# 12. Spelling & Noise Reduction (Optional)\n",
    "\n",
    "**Learning objectives**\n",
    "- Understand the challenges and risks associated with spelling correction in NLP preprocessing.\n",
    "- Explore a lightweight, naive approach using a whitelist to filter out potential noise or misspellings.\n",
    "- Recognize when to apply or skip spelling correction based on task and data characteristics.\n",
    "\n",
    "Spelling correction aims to fix typos, misspellings, or informal variations in text to standardize it for better model performance. For example, converting \"teh\" to \"the\" or \"happppy\" to \"happy\" can reduce vocabulary noise and improve token consistency. However, naive approaches (e.g., simple edit-distance algorithms or rule-based fixes) carry significant risks: they may over-correct valid terms (e.g., \"teh\" could be a name or slang), introduce errors in domain-specific jargon, or fail on context-dependent ambiguities. In noisy, informal datasets like social media, correction can distort meaning or remove stylistic elements (e.g., elongations for emphasis). Computational cost is another factor, as full correction requires dictionaries or models that slow down preprocessing.\n",
    "\n",
    "In production, specialized libraries like `pyspellchecker`, `autocorrect`, or transformer-based tools (e.g., via Hugging Face) are preferred, often combined with domain dictionaries to handle technical terms. For this educational demo, we use a very lightweight, illustrative method: a tiny whitelist of allowed words, combined with basic heuristics like dropping short or rare tokens. This simulates noise reduction without real correction, highlighting the concept's limitations. It's not recommended for real applications but serves to demonstrate pitfalls‚Äîalways test on your corpus to avoid unintended data loss. If your data is clean or spelling isn't a major issue, skip this step entirely to preserve authenticity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITELIST = {\"so\", \"happy\", \"about\", \"this\", \"ramen\", \"good\", \"cheap\", \"not\", \"sweet\"}\n",
    "\n",
    "def naive_correct(tokens):\n",
    "    # Extremely naive: drop tokens that are too rare and not in whitelist\n",
    "    return [t for t in tokens if len(t) > 2 or t in WHITELIST]\n",
    "\n",
    "examples = [\n",
    "    \"I'm soo hapy about ths ramen!\",\n",
    "    \"Ths is not good, but cheap.\",\n",
    "    \"Soo sweet and happy about it!\",\n",
    "    \"Ramen is good, not cheap tho.\"\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    tokens = spacy_tokenize(ex, lemma=False, keep_alpha=True)\n",
    "    print(f\"Original: {ex}\")\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Naive correction (demo only):\", naive_correct(tokens))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb7a17",
   "metadata": {},
   "source": [
    "# 13. Frequency-based Filtering & N-gram Construction\n",
    "\n",
    "**Learning objectives**\n",
    "- Understand how frequency-based parameters (`min_df`, `max_df`) filter terms to reduce noise and dimensionality in text vectorization.\n",
    "- Explore n-gram construction (`ngram_range`) to capture word sequences beyond single tokens, improving context for tasks like classification or topic modeling.\n",
    "- Observe the effects of these parameters on vocabulary size, feature sparsity, and model performance using scikit-learn vectorizers like `CountVectorizer` and `TfidfVectorizer`.\n",
    "\n",
    "Frequency-based filtering and n-gram construction are advanced techniques in text preprocessing that refine the feature space created by vectorizers. They help balance between capturing meaningful signals and avoiding overfitting or computational inefficiency, especially in large or noisy corpora.\n",
    "\n",
    "### N-gram Construction\n",
    "N-grams are contiguous sequences of n items (typically words or tokens) from the text. They extend beyond single words (unigrams) to include phrases, providing richer context:\n",
    "- **Unigrams (1-grams)**: Individual tokens, e.g., [\"the\", \"cat\", \"sat\"].\n",
    "- **Bigrams (2-grams)**: Pairs of consecutive tokens, e.g., [\"the cat\", \"cat sat\"].\n",
    "- **Trigrams (3-grams)**: Triples, e.g., [\"the cat sat\"].\n",
    "- Higher-order n-grams capture more context but increase dimensionality exponentially.\n",
    "\n",
    "The `ngram_range` parameter in scikit-learn vectorizers (e.g., `CountVectorizer`, `TfidfVectorizer`) specifies the range of n-grams to generate. For example:\n",
    "- `ngram_range=(1,1)`: Only unigrams.\n",
    "- `ngram_range=(1,2)`: Unigrams and bigrams.\n",
    "- `ngram_range=(2,3)`: Bigrams and trigrams.\n",
    "\n",
    "Including n-grams can improve accuracy in tasks like sentiment analysis (e.g., \"not good\" as a bigram preserves negation) or named entity recognition, but it also expands the vocabulary, potentially leading to sparsity. On small datasets, higher n-grams may overfit; on large ones, they enhance generalization.\n",
    "\n",
    "### Frequency-based Filtering\n",
    "To manage vocabulary size and remove uninformative terms, vectorizers apply document frequency thresholds:\n",
    "- **`min_df` (Minimum Document Frequency)**: Filters out terms that appear in fewer than `min_df` documents. This removes rare or noisy terms (e.g., typos or unique jargon) that don't generalize well.\n",
    "\t- Can be an integer (absolute count) or float (fraction of total documents, e.g., 0.01 for 1%).\n",
    "\t- Example: `min_df=2` excludes terms appearing in only 1 document.\n",
    "- **`max_df` (Maximum Document Frequency)**: Filters out terms that appear in more than `max_df` documents, targeting overly common terms that act like stopwords (e.g., \"the\" in most documents).\n",
    "\t- Can be an integer or float (e.g., 0.9 for 90% of documents).\n",
    "\t- Example: `max_df=0.8` removes terms in over 80% of docs, similar to custom stopword lists.\n",
    "\n",
    "These parameters reduce the feature space by eliminating low-information terms, improving computational efficiency and model interpretability. However, overly aggressive filtering (e.g., high `min_df` or low `max_df`) can discard useful signals, especially in imbalanced datasets. Always tune based on your corpus size and task‚Äîe.g., lower thresholds for sparse data, higher for noisy social media text.\n",
    "\n",
    "### Observing Effects\n",
    "In practice, experiment with these parameters to monitor changes:\n",
    "- **Vocabulary Size**: Increases with wider `ngram_range` (more n-grams) but decreases with stricter `min_df`/`max_df`.\n",
    "- **Sparsity**: Measured as the ratio of non-zero elements in the vectorized matrix. Higher n-grams and looser filters increase sparsity, potentially requiring more memory.\n",
    "- **Performance**: Use metrics like accuracy or F1-score on a holdout set to evaluate. For instance, bigrams might boost sentiment tasks but slow training.\n",
    "\n",
    "The code in the following cell demonstrates this by vectorizing the corpus with `ngram_range=(1,2)`, `min_df=1`, and `max_df=1.0`, then inspecting shapes, densities, and top TF-IDF terms. Try modifying these values (e.g., set `min_df=2` or `ngram_range=(1,3)`) to see the impact on outputs. This hands-on approach reinforces how preprocessing choices directly influence downstream NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def spacy_tokenizer_for_vectorizer(doc):\n",
    "    return remove_stopwords(spacy_tokenize(doc, lemma=True, keep_alpha=True))\n",
    "\n",
    "raw_texts = df[\"text\"].tolist()\n",
    "\n",
    "count_vec = CountVectorizer(tokenizer=spacy_tokenizer_for_vectorizer, ngram_range=(1,2), min_df=1, max_df=1.0)\n",
    "Xc = count_vec.fit_transform(raw_texts)\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=spacy_tokenizer_for_vectorizer, ngram_range=(1,2), min_df=1, max_df=1.0)\n",
    "Xt = tfidf_vec.fit_transform(raw_texts)\n",
    "\n",
    "print(\"CountVectorizer shape:\", Xc.shape, \"density:\", Xc.nnz / (Xc.shape[0]*Xc.shape[1]))\n",
    "print(\"TfidfVectorizer shape:\", Xt.shape, \"density:\", Xt.nnz / (Xt.shape[0]*Xt.shape[1]))\n",
    "\n",
    "feature_names = tfidf_vec.get_feature_names_out()\n",
    "print(\"Sample features:\", feature_names[:20])\n",
    "\n",
    "def top_k_tfidf(doc_idx=0, k=10):\n",
    "    row = Xt[doc_idx].toarray().ravel()\n",
    "    inds = np.argsort(-row)[:k]\n",
    "    return [(feature_names[i], float(row[i])) for i in inds if row[i] > 0]\n",
    "\n",
    "print(\"Top TF-IDF terms for doc 0:\", top_k_tfidf(0, k=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c898432",
   "metadata": {},
   "source": [
    "# 14. Building a Reusable Preprocessing Pipeline (Function & Class)\n",
    "\n",
    "**Learning objectives**\n",
    "- Implement a configurable function and a small class to preprocess text consistently.\n",
    "- Add simple unit-like checks to validate preprocessing outputs.\n",
    "\n",
    "Building a reusable preprocessing pipeline is essential for maintaining consistency, reproducibility, and efficiency in NLP workflows. Instead of applying preprocessing steps ad-hoc in each script or notebook, encapsulating them into a configurable function or class allows you to standardize transformations across datasets, experiments, and team members. This approach minimizes errors, facilitates debugging, and supports iterative development‚Äîe.g., easily toggling options like lemmatization or emoji handling without rewriting code.\n",
    "\n",
    "The pipeline typically chains steps from earlier sections: canonicalization, structural cleaning, normalization, tokenization, stopword removal, lemmatization, and optional handling of negations, emojis, etc. A class-based design (e.g., with `fit` and `transform` methods, inspired by scikit-learn) enables fitting on training data (e.g., for corpus-level statistics) and transforming new texts uniformly. A simpler function can suffice for stateless preprocessing. Configuration via a dictionary ensures flexibility‚Äîe.g., set `{\"lemma\": True, \"keep_negators\": False}` to customize behavior per task.\n",
    "\n",
    "To ensure reliability, include \"unit-like checks\": lightweight assertions or prints that verify outputs, such as checking for expected token counts, absence of unwanted elements (e.g., no raw URLs if masking is enabled), or vocabulary size reductions. These act as sanity checks, helping catch issues early without full model evaluation. In the demo below, we implement both a class (`Preprocessor`) and a helper function (`preprocess`), applying them to the corpus and inspecting results. This promotes best practices like modularity and testing, making your NLP pipeline robust and scalable. Always document configurations and test on edge cases (e.g., empty strings, mixed languages) to avoid surprises in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb932557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or {\n",
    "            \"mask\": True, \"lower\": True, \"strip_accents\": True,\n",
    "            \"keep_punct\": False, \"keep_digits\": False,\n",
    "            \"lemma\": True, \"keep_alpha\": True, \"keep_negators\": True,\n",
    "            \"handle_emoji\": True, \"handle_emoticon\": True, \"normalize_elong\": True\n",
    "        }\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        # Placeholder for corpus-level fitting (e.g., building slang dicts). Not needed here.\n",
    "        return self\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        out = []\n",
    "        for text in corpus:\n",
    "            t = canonicalize_text(text)\n",
    "            t = structural_clean(t, mask=self.config[\"mask\"])\n",
    "            t = handle_emojis(t) if self.config[\"handle_emoji\"] else t\n",
    "            t = handle_emoticons(t) if self.config[\"handle_emoticon\"] else t\n",
    "            if self.config[\"normalize_elong\"]:\n",
    "                t = normalize_elongations(t, max_repeat=2)\n",
    "            t = normalize_text(t,\n",
    "                               lower=self.config[\"lower\"],\n",
    "                               strip_accents=self.config[\"strip_accents\"],\n",
    "                               keep_punct=self.config[\"keep_punct\"],\n",
    "                               keep_digits=self.config[\"keep_digits\"])\n",
    "            tokens = spacy_tokenize(t, lemma=self.config[\"lemma\"], keep_alpha=self.config[\"keep_alpha\"], preserve_case=False)\n",
    "            tokens = remove_stopwords(tokens, keep_negators=self.config[\"keep_negators\"])\n",
    "            out.append(\" \".join(tokens))\n",
    "        return out\n",
    "\n",
    "def preprocess(text, cfg=None):\n",
    "    return Preprocessor(cfg).fit([text]).transform([text])[0]\n",
    "\n",
    "# Quick checks\n",
    "pp = Preprocessor().fit(df.text.tolist())\n",
    "processed = pp.transform(df.text.tolist())\n",
    "print(\"Before preprocessing:\")\n",
    "for i, text in enumerate(df.text.head()):\n",
    "    print(f\"{i+1}: {text}\")\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "for i, proc in enumerate(processed[:5]):\n",
    "    print(f\"{i+1}: {proc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2078dbb",
   "metadata": {},
   "source": [
    "\n",
    "# 15. Measuring Impact: Before/After Feature Spaces\n",
    "\n",
    "**Learning objectives**\n",
    "- Compare vocabulary size and sparsity before vs after preprocessing.\n",
    "- Visualize token counts (two separate figures).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b803920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "raw_texts = df[\"text\"].tolist()\n",
    "raw_vec = TfidfVectorizer(tokenizer=lambda s: [t.text.lower() for t in nlp(s) if not t.is_space], ngram_range=(1,1), min_df=1)\n",
    "X_raw = raw_vec.fit_transform(raw_texts)\n",
    "\n",
    "proc_texts = processed\n",
    "proc_vec = TfidfVectorizer(tokenizer=lambda s: s.split(), ngram_range=(1,1), min_df=1)\n",
    "X_proc = proc_vec.fit_transform(proc_texts)\n",
    "\n",
    "print(\"Raw vocab size:\", len(raw_vec.get_feature_names_out()), \"Sparsity:\", X_raw.nnz / (X_raw.shape[0]*X_raw.shape[1]))\n",
    "print(\"Proc vocab size:\", len(proc_vec.get_feature_names_out()), \"Sparsity:\", X_proc.nnz / (X_proc.shape[0]*X_proc.shape[1]))\n",
    "\n",
    "# Plot top token counts before\n",
    "raw_counts = Counter([t for s in raw_texts for t in [w.text.lower() for w in nlp(s) if not w.is_space]])\n",
    "def plot_top(freqs, title):\n",
    "    items = freqs.most_common(15)\n",
    "    labels = [k for k, _ in items]\n",
    "    values = [v for _, v in items]\n",
    "    plt.figure()\n",
    "    plt.bar(range(len(labels)), values)\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top(raw_counts, \"Top token counts BEFORE (raw)\")\n",
    "\n",
    "# Plot top token counts after\n",
    "proc_counts = Counter([t for s in proc_texts for t in s.split()])\n",
    "plot_top(proc_counts, \"Top token counts AFTER (processed)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11242580",
   "metadata": {},
   "source": [
    "\n",
    "# 16. Common Pitfalls, Checklists, and Best Practices\n",
    "\n",
    "**Learning objectives**\n",
    "- Recognize common errors and develop a practical checklist.\n",
    "\n",
    "**Pitfalls**\n",
    "- *Data leakage:* Fit vectorizers/transforms on training data only.\n",
    "- *Over-cleaning:* Removing negators or sentiment-bearing punctuation.\n",
    "- *Language mismatch:* Use language-appropriate models/stopwords.\n",
    "- *Domain shift:* Build domain/custom stopword lists (e.g., ‚Äúsubject‚Äù, ‚Äúhttp‚Äù in emails).\n",
    "- *Reproducibility:* Fix seeds and record configuration.\n",
    "\n",
    "**Checklist**\n",
    "- Define your task and what signal matters (e.g., sentiment vs topics).\n",
    "- Decide removal vs masking for URLs/emails/mentions/hashtags.\n",
    "- Set a consistent normalization policy (case, accents, digits, punctuation).\n",
    "- Choose lemma vs stemming and whether to keep negators.\n",
    "- Keep a compact, documented preprocessing class or config.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4839b100",
   "metadata": {},
   "source": [
    "\n",
    "# 17. Mini Review & Exercises\n",
    "\n",
    "**Learning objectives**\n",
    "- Self-assess understanding via short prompts and hands-on tasks.\n",
    "\n",
    "**Review (short answer)**\n",
    "1. When would you prefer masking over removing URLs?  \n",
    "2. Why can removing stopwords harm sentiment analysis?  \n",
    "3. Give an example where digits should be kept.  \n",
    "4. Compare lemmatization vs stemming with one example.  \n",
    "5. What is the risk of using `max_df=1.0` with bigrams on tiny corpora?  \n",
    "\n",
    "**Exercises**\n",
    "1. Add a custom tokenizer rule (e.g., keep `U.S.` as one token) and show its effect on tokens.  \n",
    "2. Tune `min_df`/`max_df` in TF-IDF and report vocabulary size changes.  \n",
    "3. Implement a variant of negation marking that stops at commas only, and compare token outputs on a few sentences.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
